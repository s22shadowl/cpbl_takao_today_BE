# scripts/generate_readme.py

import textwrap
from pathlib import Path


def generate_readme_content():
    """
    生成最新的 readme.md 內容。
    """
    # [更新] 將更新後的完整 README 內容放入此列表中
    readme_markdown_lines = [
        "# CPBL 數據後端專案",
        "",
        "## 專案總覽 (Project Overview)",
        "",
        "本專案是一個用於抓取中華職棒（CPBL）官方網站數據的後端服務，旨在提供一個通用、穩健且可擴展的數據平台。它採用容器化技術（Docker）封裝，並已成功部署至 Fly.io 雲端平台，且與 GitHub Actions 整合，實現了推送即部署的 CI/CD 流程與成本優化的自動化排程。",
        "",
        "服務核心功能是自動化爬取比賽賽程、逐場比賽的詳細攻守數據（包含所有球員的逐打席紀錄），以及球員的球季數據歷史。架構上，採用非同步任務佇列（Dramatiq + Redis）來處理耗時的爬蟲任務，並透過一個受 API 金鑰保護的 RESTful API 提供從基本查詢到複雜情境分析的多種數據需求。",
        "",
        "## 主要特色 (Features)",
        "",
        "- **通用數據抓取**: 自動抓取並儲存賽季中所有球隊、所有球員的逐打席紀錄，而非針對特定目標。",
        "- **歷史數據追蹤**: 記錄球員球季數據的歷史快照，可供分析其表現趨勢。",
        "- **高效能 API**: 透過資料庫查詢優化與應用層快取，確保複雜的分析型 API 也能快速回應。",
        "- **穩健的背景任務**: 使用 Dramatiq 與 Redis，將耗時的爬蟲工作與 API 伺服器分離。",
        "- **現代化依賴項管理**: 全面採用 Poetry 進行精確、可重複的依賴項管理，以 `pyproject.toml` 作為唯一事實來源。",
        "- **豐富的 RESTful API**: 基於 FastAPI，提供多層次的數據查詢與分析能力，並內建互動式 API 文件。",
        "- **容器化開發與部署**: 使用 Docker 與 Docker Compose 建立標準化的開發與生產環境。",
        "- **資料庫版本控制**: 使用 Alembic 管理資料庫結構的遷移。",
        "- **雲端原生架構**: 部署於 Fly.io，將 API (`web`) 與爬蟲 (`worker`) 拆分為獨立服務，並整合 `Xvfb` 繞過反爬蟲機制。",
        "- **自動化品質與安全**: 整合 pre-commit (Ruff) 與 GitHub Actions CI/CD 流程，並透過 `pip-audit` 進行依賴項安全掃描。",
        "",
        "## 生產環境架構 (Production Architecture)",
        "",
        "本專案在 Fly.io 上的生產環境由以下幾個核心元件組成：",
        "",
        "- **Fly App (`cpbl-takao-today-be`)**: 專案的主應用程式容器。",
        "  - **Web Service (`web`)**: 運行 FastAPI 的 Uvicorn 伺服器，負責接收所有 API 請求，並將耗時任務發送至佇列。",
        "  - **Worker Service (`worker`)**: 運行 Dramatiq Worker，專門執行由 `app/workers.py` 中定義的背景任務。它在 `Xvfb` 虛擬顯示環境中運行，使其能以 `headless=False` 模式操作瀏覽器，應對複雜的網站反爬蟲機制。",
        "- **Fly PostgreSQL**: 由 Fly.io 管理的獨立 PostgreSQL 資料庫服務。",
        "- **Aiven Redis**: 作為外部第三方服務，同時肩負兩種職責：",
        "  - **訊息代理 (Broker)**: 供 Dramatiq 使用 (db0)，並啟用 Result Backend 以支援任務狀態查詢。",
        "  - **應用層快取 (Cache)**: 供 Web 服務使用 (db1)，實現操作隔離。",
        "",
        "```mermaid",
        "graph TD",
        '    subgraph "使用者"',
        "        Client[外部客戶端 / 前端]",
        "    end",
        "",
        '    subgraph "GitHub Actions"',
        "        GHA{daily_crawl.yml}",
        "    end",
        "",
        '    subgraph "Fly.io 雲端平台"',
        '        subgraph "Fly App: cpbl-takao-today-be"',
        "            Web[Web Service / FastAPI]",
        '            Worker["Worker Service / Dramatiq<br/>(運行於 Xvfb 環境)"]',
        "        end",
        "        DB[(Fly PostgreSQL)]",
        "    end",
        "",
        '    subgraph "第三方服務"',
        "        Redis[(Aiven Redis db0: Broker db1: Cache)]",
        "    end",
        "",
        '    subgraph "外部網站"',
        "        CPBL[CPBL 官網]",
        "    end",
        "",
        '    GHA -- "1. 定時啟動機器" --> Web',
        '    GHA -- "1. 定時啟動機器" --> Worker',
        '    GHA -- "2. 觸發每日爬蟲 API" --> Web',
        '    GHA -- "4. 輪詢任務狀態" --> Web',
        '    GHA -- "6. 關閉機器" --> Web',
        '    GHA -- "6. 關閉機器" --> Worker',
        "",
        '    Client -- "API 請求 (HTTPS)" --> Web',
        '    Web -- "資料庫查詢" --> DB',
        '    Web -- "快取讀寫" --> Redis',
        '    Web -- "3. 發送背景任務" --> Redis',
        '    Redis -- "任務入隊" --> Worker',
        '    Worker -- "5. 執行爬蟲" --> CPBL',
        '    Worker -- "將結果寫入資料庫" --> DB',
        '    Worker -- "清除快取(可選)" --> Web',
        "```",
        "",
        "## 自動化排程與成本優化 (Automation & Cost Optimization)",
        "",
        "為了解決傳統排程器依賴服務常駐運行的問題，本專案已將排程的觸發與控制權完全轉移至 GitHub Actions，實現了按需啟停 (Scale-to-zero) 的架構，大幅降低維運成本。",
        "",
        "核心工作流程 (`.github/workflows/daily_crawl.yml`) 如下：",
        "",
        "1.  **定時啟動**: 每日固定時間，workflow 自動執行 `fly machines start` 指令，啟動 `web` 與 `worker` 機器。",
        "2.  **觸發任務**: 機器啟動後，workflow 會呼叫 `POST /api/system/trigger-daily-crawl` 端點，將每日爬蟲任務送入 Dramatiq 佇列並取得 `task_id`。",
        "3.  **主動監控**: 接著，workflow 會進入輪詢階段，週期性地呼叫 `GET /api/system/task-status/{task_id}` 端點，主動監控任務的真實執行狀態 (`succeeded`, `failed`)。",
        "4.  **即時關閉**: 一旦任務完成（無論成功或失敗），workflow 會立刻執行 `fly machines stop` 指令關閉機器，確保運算資源只在必要時運行。",
        "",
        "此架構不僅提升了排程的健壯性，更透過精準的按需控制，將服務的每日運行時間從 24 小時縮減至約 30 分鐘。",
        "",
        "### 本地測試指南",
        "",
        "若需在本地開發環境測試此流程，可依序手動模擬 GHA 的行為：",
        "",
        "1.  啟動本地服務: `docker compose up -d`",
        '2.  手動觸發任務: `curl -X POST http://127.0.0.1:8000/api/system/trigger-daily-crawl -H "X-API-Key: your_secret_api_key_here"`，並記下回傳的 `task_id`。',
        '3.  手動查詢狀態: `curl http://127.0.0.1:8000/api/system/task-status/{your_task_id} -H "X-API-Key: your_secret_api_key_here"`。',
        "4.  觀察日誌: `docker compose logs -f worker` 查看爬蟲執行狀況。",
        "",
        "## 維運與韌性 (Operations & Resilience)",
        "",
        "- **服務健康監控與自癒**: 內建 `/api/system/health` 端點，整合 Fly.io 實現服務自癒與外部告警。",
        "- **智慧重試機制**: 背景任務能自動從可恢復的網路錯誤中恢復，並區分可重試與致命錯誤，避免佇列阻塞。",
        "- **冪等性爬蟲**: 每日爬蟲任務採用「先刪除後新增」策略，確保重複執行不會造成資料重複或不一致。",
        "- **結構化日誌與追蹤**: 所有日誌均為 JSON 格式，並為每個請求注入 `request_id`，大幅簡化問題排查。",
        "",
        "## 效能優化 (Performance Optimizations)",
        "",
        "- **負載測試**: 整合 `Locust` 框架建立 API 效能基準，以科學方法識別瓶頸。",
        "- **資料庫反正規化**: 透過在 `at_bat_details` 表中新增 `game_id` 欄位，並建立複合索引，從根本上解決了昂貴的跨表 JOIN 排序問題。",
        "- **應用層快取**: 利用 Redis 為高成本的分析型 API 端點提供快取，並建立由 Worker 觸發的自動化快取失效機制，確保資料一致性。",
        "",
        "## 技術棧 (Tech Stack)",
        "",
        "| 類別                 | 技術                                     |",
        "| ---------------------- | ---------------------------------------- |",
        "| **後端框架** | FastAPI, Uvicorn                         |",
        "| **資料庫** | PostgreSQL, SQLAlchemy (ORM), Alembic    |",
        "| **背景任務 / 快取** | Dramatiq, Redis (Aiven)                  |",
        "| **網頁爬蟲** | Playwright, BeautifulSoup4, Requests     |",
        "| **容器化** | Docker, Docker Compose                   |",
        "| **雲端平台** | Fly.io                                   |",
        "| **依賴項管理** | Poetry                                   |",
        "| **CI/CD 與程式碼品質** | GitHub Actions, pre-commit, Ruff         |",
        "| **測試框架** | pytest, pytest-mock, pytest-playwright   |",
        "| **負載測試** | Locust                                   |",
        "| **設定管理** | pydantic-settings                        |",
        "| **日誌** | python-json-logger                       |",
        "| **虛擬顯示** | Xvfb (X virtual framebuffer)             |",
        "| **依賴項安全** | pip-audit                                |",
        "",
        "## 本地開發環境設定 (Local Development Setup)",
        "",
        "請遵循以下步驟在你的本地機器上設定並運行此專案。",
        "",
        "### 步驟一：取得專案程式碼",
        "",
        "```bash",
        "git clone <YOUR_REPOSITORY_URL>",
        "cd <PROJECT_DIRECTORY>",
        "```",
        "",
        "### 步驟二：設定環境變數",
        "",
        "本專案透過 `.env` 檔案管理本地開發環境的設定。請從範例檔案複製一份來開始：",
        "",
        "```bash",
        "cp .env.example .env",
        "```",
        "",
        "接著，請修改 `.env` 檔案的內容。所有必要的環境變數都定義在 `app/config.py` 中，並透過 Pydantic 進行驗證。",
        "",
        "#### 組態管理原則 (Configuration Management Principles)",
        "",
        "本專案的組態管理遵循職責分離原則：",
        "",
        "- **敏感資訊 (Secrets)**: 如 `DATABASE_URL`, `API_KEY` 等。這類資訊**絕不**能提交至版本控制。",
        "  - **生產環境**: 由 Fly.io 的 Secrets 功能管理 (`fly secrets set`)。",
        "  - **CI/CD 流程**: 由 GitHub Actions 的 Secrets 提供。",
        "  - **本地開發**: 存放於 `.env` 檔案中 (此檔案已被 `.gitignore` 排除)。",
        "",
        "- **非敏感組態 (Configuration)**: 如 `TARGET_TEAMS` 等應用層面的設定。這類資訊應受版本控制。",
        "  - **生產環境**: 定義於 `fly.toml` 的 `[env]` 區塊中。",
        "  - **本地開發**: 同樣存放於 `.env` 檔案中，方便覆寫與測試。",
        "",
        "**重要**: `fly.toml` 中 `TARGET_TEAMS` 和 `TARGET_PLAYERS` 這類列表型別的變數，**必須**使用標準的 JSON 陣列字串格式（且內部引號需轉義），以確保 Pydantic 能正確解析。",
        "",
        "### 步驟三：啟動並初始化服務",
        "",
        "本專案使用 Docker Compose 管理所有服務。",
        "",
        "1.  **啟動所有服務容器**:",
        "",
        "    ```bash",
        "    # 首次啟動或 Dockerfile/docker-compose.yml 變更後，使用 --build",
        "    docker compose up -d --build",
        "    ```",
        "",
        "2.  **初始化資料庫 (僅首次設定必要)**:",
        "    在新啟動的 `web` 容器中，使用 Alembic 建立所有必要的資料表。",
        "",
        "    ```bash",
        "    # 在 web 容器中執行 alembic upgrade 指令",
        "    docker compose run --rm worker alembic upgrade head",
        "    ```",
        "",
        "3.  **首次初始化賽程**:",
        "    使用 `curl` 或任何 API 測試工具，向以下端點發送一個 POST 請求。這會觸發背景任務，抓取整年度的賽程。",
        "",
        "    ```bash",
        "    curl -X POST [http://127.0.0.1:8000/api/update_schedule](http://127.0.0.1:8000/api/update_schedule) \\",
        '    -H "X-API-Key: your_secret_api_key_here"',
        "    ```",
        "",
        "## API 端點 (API Endpoints)",
        "",
        "本專案採用 FastAPI 框架，它會自動生成互動式的 API 文件。這份文件是查詢所有可用端點、參數及請求範例最準確的來源。",
        "",
        "當本地開發環境啟動後，請直接在瀏覽器中開啟以下任一網址：",
        "",
        "- **互動式 API 文件 (Swagger UI)**: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)",
        "",
        "- **另一種 API 文件格式 (ReDoc)**: [http://127.0.0.1:8000/redoc](http://127.0.0.1:8000/redoc)",
        "",
        "在 `/docs` 頁面中，你可以直接在線上測試每一個 API 端點，並查看其請求與回應的詳細結構。",
        "",
        "## API 整合 (API Integration)",
        "",
        "若您需要將前端應用程式或任何外部客戶端與此後端服務對接，請參考我們詳細的「[API 整合指南](docs/api_integration_guide.md)」。",
        "",
        "該指南包含了完整的認證流程、CORS 策略、標準化的錯誤碼清單，以及 API 使用範例。",
        "",
        "## 常用工具與腳本",
        "",
        "### 批次匯入歷史數據 (`bulk_import.py`)",
        "",
        "此工具已被完全容器化，用於批次抓取指定日期範圍的歷史比賽數據。**請勿在本地主機直接執行此腳本。**",
        "",
        "**指令格式:**",
        "",
        "```bash",
        'docker compose run --rm worker sh -c "Xvfb :99 -screen 0 1280x1024x24 & export DISPLAY=:99 && python -m scripts.bulk_import scrape [OPTIONS]"',
        "```",
        "",
        "**Options:**",
        "",
        "- `--start YYYY-MM-DD`: **必要**, 爬取的起始日期。",
        "",
        "- `--end YYYY-MM-DD`: 可選, 爬取的結束日期 (預設為當天)。",
        "",
        "**範例:** 爬取 2024 年 4 月 1 日至 10 日的數據",
        "",
        "```bash",
        'docker compose run --rm worker sh -c "Xvfb :99 -screen 0 1280x1024x24 & export DISPLAY=:99 && python -m scripts.bulk_import scrape --start 2024-04-01 --end 2024-04-10"',
        "```",
        "",
        "此指令會在 `worker` 容器中執行腳本，並將結果存入由 `DATABASE_URL` 指定的資料庫。",
        "",
        "## 資料庫遷移 (Database Migrations)",
        "",
        "本專案使用 Alembic 來管理資料庫結構的變更。",
        "",
        "- **當你修改 `app/models.py` 中的模型後**，你需要產生一個新的遷移腳本：",
        "",
        "  ```bash",
        "  # 在 web 容器中自動產生遷移腳本",
        '  docker compose run --rm worker alembic revision --autogenerate -m "描述你的變更"',
        "  ```",
        "",
        "- **將變更應用到資料庫**:",
        "",
        "  ```bash",
        "  # 在 web 容器中執行 upgrade",
        "  docker compose run --rm worker alembic upgrade head",
        "  ```",
        "",
        "## 程式碼品質與 CI/CD",
        "",
        "- **pre-commit**: 本專案使用 `pre-commit` 在每次 `git commit` 時自動執行 Ruff (linter + formatter)，以確保所有提交的程式碼都符合一致的風格與品質標準。初次設定請運行 `pre-commit install`。",
        "",
        "- **GitHub Actions**: 每次推送到 `main` 分支時，會觸發 CI/CD 工作流程，包含：",
        "",
        "  1. 程式碼品質與格式檢查。",
        "",
        "  2. 執行完整的 `pytest` 測試套件（排除 e2e 與 canary 測試）。",
        "",
        "  3. 若測試通過，自動部署應用至 Fly.io。",
        "",
        "### 執行特定測試",
        "",
        "本專案使用 Pytest Marker 來分類測試 (`e2e`, `canary`)。你可以使用 `-m` 參數來篩選要執行或跳過的測試。",
        "",
        "- **僅執行單元測試 (CI 的主要流程)**:",
        "",
        "  ```bash",
        '  pytest -m "not e2e and not canary"',
        "  ```",
        "",
        "- **僅執行 e2e 測試**:",
        "",
        "  ```bash",
        "  pytest -m e2e",
        "  ```",
        "",
        "- **僅執行金絲雀測試**:",
        "",
        "  ```bash",
        "  pytest -m canary",
        "  ```",
        "",
        "## 部署 (Deployment)",
        "",
        "### 自動化部署",
        "",
        "本專案已設定 CI/CD，任何推送到 `main` 分支且通過所有測試的提交，都會被自動部署到 Fly.io。這是標準的部署流程。",
        "",
        "### 備用路徑：手動部署 (用於緊急修復或特殊情況)",
        "",
        "在 CI/CD 流程無法使用或需要緊急部署時，可以透過 `flyctl` CLI 工具手動部署。",
        "",
        "1. **設定秘密 (Secrets)**:",
        "   在首次部署或 Secret 變更時，你仍需手動設定生產環境的敏感資訊。這些資訊**絕不**應存放在版本控制中。",
        "",
        "   ```bash",
        "   fly secrets set \\",
        '     DATABASE_URL="<YOUR_FLY_POSTGRES_URL>" \\',
        '     DRAMATIQ_BROKER_URL="<YOUR_AIVEN_REDIS_URL>" \\',
        '     API_KEY="<YOUR_PRODUCTION_API_KEY>" \\',
        "   ```",
        "",
        "2. **部署應用**:",
        "   設定完 secrets 後，在專案根目錄執行以下指令即可觸發部署。",
        "",
        "   ```bash",
        "   fly deploy",
        "   ```",
        "",
        "   `flyctl` 會讀取 `fly.toml` 檔案，在本機建置 Docker 映像檔，並將其推送到 Fly.io 平台，更新 `web` 與 `worker` 服務。",
        "",
        "## 本地日誌查看技巧 (Local Log Viewing)",
        "",
        "由於主控台日誌已改為結構化的 JSON 格式，建議搭配使用命令列工具 `jq` 來美化、上色及過濾日誌。",
        "",
        "**前置需求**:",
        "請先確保你的系統已安裝 `jq` (例如在 Ubuntu/WSL 中執行 `sudo apt-get install jq`)。",
        "",
        "**美化輸出**:",
        "",
        "```bash",
        "# 查看 web 服務的美化日誌",
        "docker compose logs web | jq",
        "",
        "# 持續追蹤 worker 服務的美化日誌",
        "docker compose logs -f worker | jq",
        "```",
        "",
        "**依 request_id 過濾**:",
        "當你需要追蹤單一 API 請求的完整生命週期時，可利用我們新增的 `request_id` 欄位進行過濾。",
        "",
        "```bash",
        "# 只顯示特定 request_id 的日誌記錄",
        "docker compose logs web | jq 'select(.request_id == \"YOUR_REQUEST_ID_HERE\")'",
        "```",
    ]

    # 使用 dedent 來處理多行字串的縮排，並用 join 來合併成單一字串
    readme_markdown = textwrap.dedent("\n".join(readme_markdown_lines)).strip()

    # 計算出專案根目錄的路徑
    project_root = Path(__file__).parent.parent
    output_file_path = project_root / "README.md"

    # 將內容寫入檔案，使用 "w" 模式會直接覆蓋
    try:
        with open(output_file_path, "w", encoding="utf-8") as f:
            f.write(readme_markdown)
            f.write("\n")  # 確保檔案結尾有換行符

        # 在終端機回報成功訊息
        print(f"✅ README.md has been successfully updated at: {output_file_path}")

    except IOError as e:
        print(f"❌ Error writing to file {output_file_path}: {e}")


if __name__ == "__main__":
    generate_readme_content()
